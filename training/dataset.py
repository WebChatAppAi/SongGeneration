import torch
from torch.utils.data import Dataset, DataLoader
import json
import os

class SongDataset(Dataset):
    """
    PyTorch Dataset for loading pre-processed song data.
    Reads a manifest file to find the locations of tokenized data files.
    """
    def __init__(self, manifest_path):
        """
        Args:
            manifest_path (str): Path to the manifest.jsonl file generated by prepare_data.py.
        """
        super().__init__()
        self.manifest_path = manifest_path
        self.entries = []
        if not os.path.exists(manifest_path):
            raise FileNotFoundError(f"Manifest file not found at {manifest_path}. "
                                    "Please run prepare_data.py first.")

        with open(manifest_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.entries.append(json.loads(line))

    def __len__(self):
        """Returns the total number of samples in the dataset."""
        return len(self.entries)

    def __getitem__(self, idx):
        """
        Loads and returns a single sample from the dataset.

        Args:
            idx (int): The index of the sample to retrieve.

        Returns:
            dict: A dictionary containing the token tensor, lyrics, and descriptions.
                  e.g., {'tokens': tensor, 'lyrics': str, 'descriptions': str}
        """
        entry = self.entries[idx]
        token_path = entry['token_path']

        try:
            data = torch.load(token_path)
            return data
        except FileNotFoundError:
            print(f"Warning: Token file not found at {token_path}. Skipping.")
            # Return the next valid sample to avoid crashing the training loop
            return self.__getitem__((idx + 1) % len(self))
        except Exception as e:
            print(f"Warning: Error loading {token_path}: {e}. Skipping.")
            return self.__getitem__((idx + 1) % len(self))

def collate_fn(batch):
    """
    Custom collate function to batch data from the SongDataset.

    Args:
        batch (list): A list of dictionaries, where each dictionary is a sample
                      from SongDataset.__getitem__.

    Returns:
        dict: A dictionary containing the batched data.
              e.g., {'tokens': torch.Tensor, 'lyrics': list[str], 'descriptions': list[str]}
    """
    tokens = [item['tokens'] for item in batch]
    lyrics = [item['lyrics'] for item in batch]
    descriptions = [item['descriptions'] for item in batch]

    # Stack tokens into a single batch tensor
    batched_tokens = torch.stack(tokens, dim=0)

    return {
        'tokens': batched_tokens,
        'lyrics': lyrics,
        'descriptions': descriptions
    }

if __name__ == '__main__':
    # This is a demonstration of how to use the SongDataset and DataLoader.
    # To run this, you must first run prepare_data.py to create the dataset.

    # Create a dummy manifest and dummy data for testing purposes
    print("Creating dummy data for demonstration...")
    dummy_data_dir = 'dummy_prepared_data'
    os.makedirs(dummy_data_dir, exist_ok=True)
    dummy_manifest_path = os.path.join(dummy_data_dir, 'manifest.jsonl')

    with open(dummy_manifest_path, 'w') as f:
        for i in range(10):
            dummy_token_path = os.path.join(dummy_data_dir, f'{i}.pt')
            dummy_data = {
                'tokens': torch.randint(0, 1024, (3, 750)), # 3 codebooks, 750 time steps
                'lyrics': f'[verse] This is dummy lyric line {i}.',
                'descriptions': f'pop, female singer, bpm is {120+i}'
            }
            torch.save(dummy_data, dummy_token_path)

            manifest_entry = {
                'audio_path': f'/path/to/dummy/{i}.wav',
                'token_path': dummy_token_path,
                'idx': i
            }
            f.write(json.dumps(manifest_entry) + '\n')

    print("Dummy data created.")

    # --- Demonstration ---
    print("\n--- DataLoader Demonstration ---")

    # 1. Instantiate the Dataset
    try:
        dataset = SongDataset(manifest_path=dummy_manifest_path)
        print(f"Dataset loaded successfully with {len(dataset)} samples.")

        # 2. Instantiate the DataLoader
        dataloader = DataLoader(
            dataset=dataset,
            batch_size=4,
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=0 # Set to 0 for this simple demo
        )
        print("DataLoader created.")

        # 3. Iterate over a batch
        print("\nFetching one batch from the DataLoader...")
        first_batch = next(iter(dataloader))

        # 4. Inspect the batch
        print("Batch inspection:")
        print(f"  - Type of batch: {type(first_batch)}")
        print(f"  - Keys in batch: {first_batch.keys()}")

        tokens_batch = first_batch['tokens']
        lyrics_batch = first_batch['lyrics']
        descriptions_batch = first_batch['descriptions']

        print(f"  - Shape of 'tokens' tensor: {tokens_batch.shape}")
        print(f"  - Type of 'tokens' tensor: {tokens_batch.dtype}")
        print(f"  - Number of lyrics in batch: {len(lyrics_batch)}")
        print(f"  - First lyric: '{lyrics_batch[0]}'")
        print(f"  - Number of descriptions in batch: {len(descriptions_batch)}")
        print(f"  - First description: '{descriptions_batch[0]}'")

    except Exception as e:
        print(f"\nAn error occurred during the demonstration: {e}")
        print("Please ensure you have run 'prepare_data.py' to generate a real dataset, or fix the dummy data creation.")
    finally:
        # Clean up dummy files
        import shutil
        if os.path.exists(dummy_data_dir):
            shutil.rmtree(dummy_data_dir)
            print(f"\nCleaned up dummy data directory: {dummy_data_dir}")
